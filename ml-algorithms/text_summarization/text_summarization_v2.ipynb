{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge import Rouge\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Rouge for evaluation\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def get_sentence_scores(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Preprocess sentences\n",
    "        processed_sentences = [self.preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "        # Calculate TF-IDF scores\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "        # Calculate sentence scores based on TF-IDF weights\n",
    "        sentence_scores = []\n",
    "        for i in range(len(sentences)):\n",
    "            score = np.mean(tfidf_matrix[i].toarray())\n",
    "            sentence_scores.append((sentences[i], score))\n",
    "\n",
    "        return sentence_scores\n",
    "\n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Get sentence scores\n",
    "        sentence_scores = self.get_sentence_scores(text)\n",
    "\n",
    "        # Sort sentences by score\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select top n sentences\n",
    "        selected_sentences = [sentence[0] for sentence in sentence_scores[:num_sentences]]\n",
    "\n",
    "        # Sort sentences by their original position\n",
    "        original_sentences = sent_tokenize(text)\n",
    "        final_sentences = [sent for sent in original_sentences if sent in selected_sentences]\n",
    "\n",
    "        return ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_summarizer import TextSummarizer\n",
    "\n",
    "# Train and save the summarizer\n",
    "summarizer = TextSummarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset and train if needed\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the summarizer...\n"
     ]
    }
   ],
   "source": [
    "# Train the summarizer\n",
    "print(\"Training the summarizer...\")\n",
    "train_sample = train_df['article'].tolist()  \n",
    "for text in train_sample:\n",
    "    summarizer.summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the summarizer...\n",
      "\n",
      "Validation ROUGE Scores:\n",
      "========================================\n",
      "\n",
      "ROUGE-1:\n",
      "--------------------\n",
      "Precision: 0.2207\n",
      "Recall:    0.4000\n",
      "F1-Score:  0.2762\n",
      "\n",
      "ROUGE-2:\n",
      "--------------------\n",
      "Precision: 0.0721\n",
      "Recall:    0.1376\n",
      "F1-Score:  0.0911\n",
      "\n",
      "ROUGE-L:\n",
      "--------------------\n",
      "Precision: 0.1997\n",
      "Recall:    0.3616\n",
      "F1-Score:  0.2498\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on a sample of validation data\n",
    "print(\"Evaluating the summarizer...\")\n",
    "val_sample = val_df\n",
    "val_summaries = [summarizer.summarize(text, num_sentences=3) for text in val_sample['article'].values]\n",
    "val_highlights = val_sample['highlights'].tolist()\n",
    "val_scores = rouge_scorer.get_scores(val_summaries, val_highlights, avg=True)\n",
    "\n",
    "# Print scores in a formatted way\n",
    "print(\"\\nValidation ROUGE Scores:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for rouge_type, scores in val_scores.items():\n",
    "    print(f\"\\n{rouge_type.upper()}:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Precision: {scores['p']:.4f}\")\n",
    "    print(f\"Recall:    {scores['r']:.4f}\")\n",
    "    print(f\"F1-Score:  {scores['f']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_summarizer_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(summarizer, 'text_summarizer_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the saved model...\n",
      "Test Summary: I am writing to express my deep frustration and concern regarding the deplorable condition of the main road in our neighborhood. The lack of maintenance and timely repairs is unacceptable and reflects poorly on the administration's commitment to public safety.\n"
     ]
    }
   ],
   "source": [
    "# Test loading and using the model\n",
    "print(\"\\nTesting the saved model...\")\n",
    "loaded_summarizer = joblib.load('text_summarizer_model.joblib')\n",
    "test_text = \"I am writing to express my deep frustration and concern regarding the deplorable condition of the main road in our neighborhood. Over the past few months, the road has deteriorated significantly, and it is now riddled with numerous potholes of varying sizes. These potholes pose a serious hazard to both drivers and pedestrians. Every day, I witness vehicles swerving dangerously to avoid these craters, which increases the risk of accidents. The situation is particularly dire during the rainy season when the potholes fill with water, making them even more difficult to see and navigate. This not only damages vehicles but also endangers the lives of those who use the road. Despite multiple complaints to the local authorities, no action has been taken to repair the road. The lack of maintenance and timely repairs is unacceptable and reflects poorly on the administration's commitment to public safety. I urge the concerned authorities to prioritize the repair of this road to prevent any further accidents and ensure the safety of all residents.\"\n",
    "summary = loaded_summarizer.summarize(test_text, num_sentences=2)\n",
    "print(\"Test Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
