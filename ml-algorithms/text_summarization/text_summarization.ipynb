{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rouge import Rouge\n",
    "import joblib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractiveTextSummarizer:\n",
    "    def __init__(self):\n",
    "        # Download required NLTK data\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.rouge = Rouge()\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def fit_transform_vectorizer(self, texts, batch_size=1000):\n",
    "        tfidf_matrix = None\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            processed_batch = [self.preprocess_text(text) for text in batch]\n",
    "            batch_matrix = self.vectorizer.fit_transform(processed_batch)\n",
    "            if tfidf_matrix is None:\n",
    "                tfidf_matrix = batch_matrix\n",
    "            else:\n",
    "                tfidf_matrix = np.vstack((tfidf_matrix, batch_matrix))\n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def get_sentence_scores(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Preprocess sentences\n",
    "        processed_sentences = [self.preprocess_text(sentence) for sentence in sentences]\n",
    "        \n",
    "        # Calculate TF-IDF scores\n",
    "        tfidf_matrix = self.fit_transform_vectorizer(processed_sentences)\n",
    "        \n",
    "        # Calculate sentence scores based on TF-IDF weights\n",
    "        sentence_scores = []\n",
    "        for i in range(len(sentences)):\n",
    "            score = np.mean(tfidf_matrix[i].toarray())\n",
    "            sentence_scores.append((sentences[i], score))\n",
    "            \n",
    "        return sentence_scores\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Get sentence scores\n",
    "        sentence_scores = self.get_sentence_scores(text)\n",
    "        \n",
    "        # Sort sentences by score\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Select top n sentences\n",
    "        selected_sentences = [sentence[0] for sentence in sentence_scores[:num_sentences]]\n",
    "        \n",
    "        # Sort sentences by their original position\n",
    "        original_sentences = sent_tokenize(text)\n",
    "        final_sentences = [sent for sent in original_sentences if sent in selected_sentences]\n",
    "        \n",
    "        return ' '.join(final_sentences)\n",
    "    \n",
    "    def train(self, train_data, val_data, batch_size=1000):\n",
    "        # Fit the vectorizer on the training data in batches\n",
    "        all_train_texts = train_data['article'].tolist()\n",
    "        self.fit_transform_vectorizer(all_train_texts, batch_size=batch_size)\n",
    "        \n",
    "        # Evaluate on validation data\n",
    "        val_summaries = [self.summarize(text, num_sentences=3) for text in val_data['article'].values]\n",
    "        val_highlights = val_data['highlights'].tolist()\n",
    "        val_scores = self.rouge.get_scores(val_summaries, val_highlights, avg=True)\n",
    "        print(\"Validation ROUGE Scores:\", val_scores)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        joblib.dump(self, filename)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_model(filename):\n",
    "        return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def fit_transform_vectorizer(texts, batch_size=1000):\n",
    "    tfidf_matrix = None\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        processed_batch = [preprocess_text(text) for text in batch]\n",
    "        batch_matrix = vectorizer.fit_transform(processed_batch)\n",
    "        if tfidf_matrix is None:\n",
    "            tfidf_matrix = batch_matrix\n",
    "        else:\n",
    "            tfidf_matrix = np.vstack((tfidf_matrix, batch_matrix))\n",
    "    return tfidf_matrix\n",
    "\n",
    "def get_sentence_scores(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Preprocess sentences\n",
    "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    tfidf_matrix = fit_transform_vectorizer(processed_sentences)\n",
    "    \n",
    "    # Calculate sentence scores based on TF-IDF weights\n",
    "    sentence_scores = []\n",
    "    for i in range(len(sentences)):\n",
    "        score = np.mean(tfidf_matrix[i].toarray())\n",
    "        sentence_scores.append((sentences[i], score))\n",
    "        \n",
    "    return sentence_scores\n",
    "\n",
    "def summarize(text, num_sentences=3):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Get sentence scores\n",
    "    sentence_scores = get_sentence_scores(text)\n",
    "    \n",
    "    # Sort sentences by score\n",
    "    sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top n sentences\n",
    "    selected_sentences = [sentence[0] for sentence in sentence_scores[:num_sentences]]\n",
    "    \n",
    "    # Sort sentences by their original position\n",
    "    original_sentences = sent_tokenize(text)\n",
    "    final_sentences = [sent for sent in original_sentences if sent in selected_sentences]\n",
    "    \n",
    "    return ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_transform_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the summarizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m all_train_texts \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfit_transform_vectorizer\u001b[49m(all_train_texts, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fit_transform_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the summarizer\n",
    "all_train_texts = train_df['article'].tolist()\n",
    "fit_transform_vectorizer(all_train_texts, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE Scores: {'rouge-1': {'r': 0.3999497644136857, 'p': 0.22068170337896695, 'f': 0.27617508641236926}, 'rouge-2': {'r': 0.1376090268116563, 'p': 0.07204719465355827, 'f': 0.09109734677975852}, 'rouge-l': {'r': 0.3615805745357685, 'p': 0.1996810625084078, 'f': 0.24978781526751093}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data\n",
    "val_summaries = [summarize(text, num_sentences=3) for text in val_df['article'].values]\n",
    "val_highlights = val_df['highlights'].tolist()\n",
    "val_scores = rouge.get_scores(val_summaries, val_highlights, avg=True)\n",
    "print(\"Validation ROUGE Scores:\", val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_summarizer_v3.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(summarize, 'text_summarizer_v3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92c514c913c0bdfe25341af9fd72b29db544099b</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003841c7dc0e7c5b1a248f9cd536d727f27a45a</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91b7d2311527f5c2b63a65ca98d21d9c92485149</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>caabf9cbdf96eb1410295a673e953d304391bfbb</td>\n",
       "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
       "      <td>Fiorentina goalkeeper Neto has been linked wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3da746a7d9afcaa659088c8366ef6347fe6b53ea</td>\n",
       "      <td>Bruce Jenner will break his silence in a two-h...</td>\n",
       "      <td>Tell-all interview with the reality TV star, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
       "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
       "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
       "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
       "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
       "\n",
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "3  Liverpool target Neto is also wanted by PSG an...   \n",
       "4  Bruce Jenner will break his silence in a two-h...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Experts question if  packed out planes are put...  \n",
       "1  Drunk teenage boy climbed into lion enclosure ...  \n",
       "2  Nottingham Forest are close to extending Dougi...  \n",
       "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
       "4  Tell-all interview with the reality TV star, 6...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yakuma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROUGE Scores: {'rouge-1': {'r': 0.3999497644136857, 'p': 0.22068170337896695, 'f': 0.27617508641236926}, 'rouge-2': {'r': 0.1376090268116563, 'p': 0.07204719465355827, 'f': 0.09109734677975852}, 'rouge-l': {'r': 0.3615805745357685, 'p': 0.1996810625084078, 'f': 0.24978781526751093}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ExtractiveTextSummarizer at 0x23cbc7c3040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the summarizer\n",
    "# Train the summarizer\n",
    "summarizer = ExtractiveTextSummarizer()\n",
    "summarizer.train(train_df, val_df, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_summarizerv2.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "# summarizer.save_model('text_summarizer.joblib')\n",
    "joblib.dump(summarizer, 'text_summarizerv2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "summarizer = joblib.load('text_summarizer_v3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Potholes are not a big issues. Potholes are big issue.\n",
      "Original Summary: The main road in our neighborhood is full of dangerous potholes, posing serious risks to drivers and pedestrians, and despite multiple complaints, no repairs have been made.\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "test_text = 'Potholes are not a big issues. But it is. Potholes are big issue.'\n",
    "summary = summarizer(test_text, num_sentences=2)\n",
    "print(\"Summary:\", summary)\n",
    "# print(\"Original Text:\", test_text)\n",
    "print(\"Original Summary:\", 'The main road in our neighborhood is full of dangerous potholes, posing serious risks to drivers and pedestrians, and despite multiple complaints, no repairs have been made.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
